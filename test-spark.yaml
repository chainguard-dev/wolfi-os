package:
  name: sparkr
  version: 3.5.3
  epoch: 1
  description: Unified engine for large-scale data analytics
  copyright:
    - license: Apache-2.0
  dependencies:
    provides:
      - spark=${{package.full-version}}
      - spark-3.5=${{package.full-version}}
      - spark-3.5-scala-2.13=${{package.full-version}}
    runtime:
      - openjdk-17

environment:
  contents:
    packages:
      - bash
      - curl
      - maven
      - openjdk-17-default-jdk
      - wolfi-base
      - libstdc++-11
      - libavcodec61
  environment:
    LANG: en_US.UTF-8
    JAVA_HOME: /usr/lib/jvm/java-17-openjdk
    PATH: $PATH:$JAVA_HOME/bin
    SPARK_HOME: /usr/lib/spark
    SCALA_VERSION: 2.13

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/apache/spark
      tag: v${{package.version}}
      expected-commit: 32232e9ed33bb16b93ad58cfde8b82e0f07c0970

  - runs: |
      mvn clean
      ./dev/change-scala-version.sh 2.13
      ./build/mvn -DskipTests -Pscala-2.13 clean package
      mkdir -p ${{targets.contextdir}}/usr/lib/spark
      mkdir -p ${{targets.contextdir}}/usr/lib/spark/work-dir
      mv bin/ ${{targets.contextdir}}/usr/lib/spark
      mv sbin/ ${{targets.contextdir}}/usr/lib/spark
      mv target ${{targets.contextdir}}/usr/lib/spark
      mv assembly ${{targets.contextdir}}/usr/lib/spark

test:
  environment:
    contents:
      packages:
        - python3
        - openjdk-17-default-jvm
        - R
        - bash
    environment:
      LANG: en_US.UTF-8
      SCALA_VERSION: 2.13
      JAVA_HOME: /usr/lib/jvm/java-17-openjdk
      SPARK_LOCAL_IP: 127.0.0.1
      SPARK_LOCAL_HOSTNAME: localhost
      SPARK_HOME: /usr/lib/spark

  pipeline:
    - name: Test ${{package.name}} with OpenJDK 17
      pipeline:
        - name: Test if the Scala versions are correct
          runs: ls /usr/lib/spark/assembly/target/scala-2.13/jars/scala-* | grep -q $SCALA_VERSION
        - name: Check spark-shell --version
          runs: /usr/lib/spark/bin/spark-shell --version
        - name: Check spark-submit --version
          runs: /usr/lib/spark/bin/spark-submit --version
        - name: Check pyspark --version
          runs: /usr/lib/spark/bin/pyspark --version
        - name: Check spark-sql --version
          runs: /usr/lib/spark/bin/spark-sql --version
        - name: Test entrypoint
          runs: timeout 10 /opt/entrypoint.sh /opt/spark/bin/spark-shell > spark_log.txt 2>&1 || [ $? -eq 143 ] && grep "Spark session available" spark_log.txt && exit_code=0 || exit_code=$? && echo $exit_code
        - name: Run a simple Scala test script
          runs: |
            cat <<EOF > SimpleJob.scala
            val data = Seq(1, 2, 3, 4, 5)
            val rdd = spark.sparkContext.parallelize(data)
            val sum = rdd.reduce(_ + _)
            assert(sum == 15)
            EOF
            cat SimpleJob.scala | /usr/lib/spark/bin/spark-shell
        - name: Run a simple Spark job in Python
          runs: |
            cat <<EOF > simple_job.py
            from pyspark.sql import SparkSession
            spark = SparkSession.builder.appName("SimpleJob").getOrCreate()
            data = [1, 2, 3, 4, 5]
            rdd = spark.sparkContext.parallelize(data)
            sum = rdd.reduce(lambda x, y: x + y)
            assert sum == 15
            EOF
            /usr/lib/spark/bin/spark-submit simple_job.py --jars /usr/lib/spark/jars/guava-32.0.1-jre.jar
        - name: Perform SQL query on DataFrame in Scala
          runs: |
            cat <<EOF > SQLTest.scala
            import org.apache.spark.sql.SparkSession
            val spark = SparkSession.builder.appName("SQLTest").getOrCreate()
            import spark.implicits._
            val df = Seq((1, "Alice"), (2, "Bob")).toDF("id", "name")
            df.createOrReplaceTempView("people")
            val result = spark.sql("SELECT name FROM people WHERE id = 2")
            assert(result.count() == 1 && result.first().getString(0) == "Bob")
            EOF
            cat SQLTest.scala | /usr/lib/spark/bin/spark-shell
        - name: Run basic SQL query with spark-sql
          runs: |
            echo 'CREATE OR REPLACE TEMP VIEW test AS SELECT 1 AS id;' > test.sql
            echo 'SELECT * FROM test;' >> test.sql
            /usr/lib/spark/bin/spark-sql -f test.sql
        - name: Run SparkR script
          runs: |
            cat <<EOF > sparkr_test.R
            library(SparkR)
            sparkR.session()
            df <- as.DataFrame(data.frame(id = c(1, 2), name = c("Alice", "Bob")))
            createOrReplaceTempView(df, "people")
            df2 <- sql("SELECT * FROM people WHERE id > 1")
            stopifnot(count(df2) == 1)
            EOF
            /usr/lib/spark/bin/spark-submit sparkr_test.R
