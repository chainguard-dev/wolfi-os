package:
  name: airflow-core
  version: 2.10.4
  epoch: 0
  description: Minimal Airflow core - Platform to programmatically author, schedule, and monitor workflows
  dependencies:
    runtime:
      - bash
      - busybox
      - coreutils
      - curl
      - cyrus-sasl
      - cyrus-sasl-dev
      - findutils
      - glibc-locale-en
      - glibc-locales
      - gnupg-utils
      - gzip
      - mariadb-dev
      - openssl
      - postgresql-client
      - postgresql-dev
      - python-${{vars.py-version}}
  copyright:
    - license: Apache-2.0

# As of now airflow is primarily available for python 3.12 and will break for python 3.13
# For ref: https://airflow.apache.org/blog/airflow-2.9.0
vars:
  py-version: 3.12

environment:
  contents:
    packages:
      - bash
      - busybox
      - gcc
      - glibc-dev
      - mariadb-connector-c-dev
      - mariadb-dev
      - openssf-compiler-options
      - pkgconf-dev
      - postgresql-dev
      - py${{vars.py-version}}-build-bin
      - py${{vars.py-version}}-pip
      - python-${{vars.py-version}}
      - python-${{vars.py-version}}-dev
      - zlib-dev
  environment:
    # This is needed to work around the error "ValueError: ZIP does not support timestamps before 1980"
    SOURCE_DATE_EPOCH: 315532800

pipeline:
  - uses: git-checkout
    with:
      repository: https://github.com/apache/airflow
      tag: ${{package.version}}
      expected-commit: c083e456fa02c6cb32cdbe0c9ed3c3b2380beccd

  - runs: |
      # Install airflow core, the upstream sets this as directory
      export PYTHONUSERBASE=${{targets.destdir}}/home/airflow/.local
      pip install --no-cache-dir --user apache-airflow[core]==${{package.version}}

      #GHSA-8w49-h785-mj3c/GHSA-8495-4g3g-x7pr/GHSA-27mf-ghqm-j3j8 fixes
      pip install --no-cache-dir --user aiohttp>=3.10.11 tornado>=6.4.2
      pip install --no-cache-dir --user --force-reinstall packaging statsd

      # CVE-2024-6345 GHSA-cx63-2mw6-8hw5
      # setuptools comes from airflow/providers/google/provider.yaml having
      # gcloud-aio-auth>=4.0.0,<5.0.0 . gcloud-aio-auth 4 is backlevel and has
      # setuptools in it's pyproject.toml 'tool.poetry.dependencies'
      # The tldr; For that case it is not needed in runtime.
      pip uninstall --yes setuptools

      find . -name '__pycache__' -exec rm -rf {} +

  - runs: |
      mkdir -p ${{targets.destdir}}/opt/airflow/dags
      mkdir -p ${{targets.destdir}}/opt/airflow/logs
      mkdir -p ${{targets.destdir}}/scripts/docker

      # The first time you run Airflow, it will create a file called `airflow.cfg` in
      # `$AIRFLOW_HOME` directory
      # However, for production case it is advised to generate the configuration
      export PYTHONPATH=${{targets.destdir}}/home/airflow/.local/lib/python${{vars.py-version}}/site-packages

      ${{targets.destdir}}/home/airflow/.local/bin/airflow config list --defaults > ${{targets.destdir}}/opt/airflow/"airflow.cfg"

      cp airflow/config_templates/default_webserver_config.py ${{targets.destdir}}/

      cp scripts/docker/entrypoint_prod.sh ${{targets.destdir}}/entrypoint
      chmod 755 ${{targets.destdir}}/entrypoint

      cp scripts/docker/clean-logs.sh ${{targets.destdir}}/clean-logs
      chmod 755 ${{targets.destdir}}/clean-logs

      cp scripts/docker/airflow-scheduler-autorestart.sh ${{targets.destdir}}/airflow-scheduler-autorestart
      chmod 755 ${{targets.destdir}}/airflow-scheduler-autorestart

      # unlike airflow, airflow-core needs only these scripts
      cp scripts/docker/common.sh ${{targets.destdir}}/scripts/docker
      cp scripts/docker/install_mssql.sh ${{targets.destdir}}/scripts/docker
      cp scripts/docker/install_mysql.sh ${{targets.destdir}}/scripts/docker
      cp scripts/docker/install_os_dependencies.sh ${{targets.destdir}}/scripts/docker
      cp scripts/docker/install_postgres.sh ${{targets.destdir}}/scripts/docker
      chmod 755 ${{targets.destdir}}/scripts/docker/*

# We re-use airflow-compat package to symlink libstdc++ from usr/lib to /usr/lib/$(uname -m)-linux-gnu/
# And create this oci-entrypoint subpackage to have it compliant with upstream.
subpackages:
  - name: ${{package.name}}-oci-entrypoint
    description: Entrypoint for using airflow-core in OCI containers
    dependencies:
      runtime:
        - ${{package.name}}
    pipeline:
      - runs: |
          mkdir -p ${{targets.contextdir}}/opt/airflow/dags
          mkdir -p ${{targets.contextdir}}/opt/airflow/logs
          mkdir -p ${{targets.contextdir}}/scripts/docker

          # The first time you run Airflow, it will create a file called `airflow.cfg` in
          # `$AIRFLOW_HOME` directory
          # However, for production case it is advised to generate the configuration
          export PYTHONPATH=${{targets.destdir}}/home/airflow/.local/lib/python${{vars.py-version}}/site-packages

          ${{targets.destdir}}/home/airflow/.local/bin/airflow config list --defaults > ${{targets.contextdir}}/opt/airflow/"airflow.cfg"

          cp airflow/config_templates/default_webserver_config.py ${{targets.contextdir}}/

          cp scripts/docker/entrypoint_prod.sh ${{targets.contextdir}}/entrypoint
          chmod 755 ${{targets.contextdir}}/entrypoint

          cp scripts/docker/clean-logs.sh ${{targets.contextdir}}/clean-logs
          chmod 755 ${{targets.contextdir}}/clean-logs

          cp scripts/docker/airflow-scheduler-autorestart.sh ${{targets.contextdir}}/airflow-scheduler-autorestart
          chmod 755 ${{targets.contextdir}}/airflow-scheduler-autorestart

          # unlike airflow, airflow-core needs only these scripts
          cp scripts/docker/common.sh ${{targets.contextdir}}/scripts/docker
          cp scripts/docker/install_mssql.sh ${{targets.contextdir}}/scripts/docker
          cp scripts/docker/install_mysql.sh ${{targets.contextdir}}/scripts/docker
          cp scripts/docker/install_os_dependencies.sh ${{targets.contextdir}}/scripts/docker
          cp scripts/docker/install_postgres.sh ${{targets.contextdir}}/scripts/docker
          chmod 755 ${{targets.contextdir}}/scripts/docker/*

update:
  enabled: true
  ignore-regex-patterns:
    - 'rc\d+$'
    - 'helm-chart*'
  github:
    identifier: apache/airflow

test:
  environment:
    contents:
      packages:
        - python-${{vars.py-version}}
        - airflow-compat
        - bash
  pipeline:
    - runs: |
        # Set environment variables
        export PATH=/home/airflow/.local/bin:$PATH
        export PYTHONPATH=/home/airflow/.local/lib/python${{vars.py-version}}/site-packages
        export AIRFLOW_HOME=/opt/airflow

        # Initialize Airflow database
        airflow db init

        # Create and test a simple DAG
        echo "Creating and testing a simple DAG..."
        mkdir -p /opt/airflow/dags
        cat <<EOF > /opt/airflow/dags/test_dag.py
        from airflow import DAG
        from airflow.operators.bash import BashOperator
        from datetime import datetime

        with DAG("test_dag", start_date=datetime(2024, 1, 1), schedule_interval=None) as dag:
            task = BashOperator(
                task_id="test_task",
                bash_command="echo Hello World!"
            )
        EOF

        # List DAGs
        airflow dags list | grep test_dag || (echo "DAG not found!" && exit 1)

        # Test Task Execution in DAG
        airflow tasks test test_dag test_task 2024-01-01 || (echo "Task execution failed!" && exit 1)
